{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ncg/.local/share/virtualenvs/atlas-f2U08CBN/lib/python3.6/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "# needs a separate library in order to access the site b/c BS4 \n",
    "#    isn't an HTTP client\n",
    "import urllib.request as url\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# stuff to access the Google API client\n",
    "import pickle\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this accesses the google database I use to get geographic info\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly']\n",
    "SAMPLE_SPREADSHEET_ID = '19X7PwbLrKLGJySaFSHr5Pze-VRM-6l_9EOqXDu43T3k'\n",
    "SAMPLE_RANGE_NAME = 'Sheet1!A1:E171'\n",
    "\n",
    "creds = None\n",
    "# The file token.pickle stores the user's access and refresh tokens, and is\n",
    "# created automatically when the authorization flow completes for the first\n",
    "# time.\n",
    "if os.path.exists('token.pickle'):\n",
    "    with open('token.pickle', 'rb') as token:\n",
    "        creds = pickle.load(token)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\n",
    "            'credentials.json', SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open('token.pickle', 'wb') as token:\n",
    "        pickle.dump(creds, token)\n",
    "\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "sheet = service.spreadsheets()\n",
    "\n",
    "result = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "                                range=SAMPLE_RANGE_NAME).execute()\n",
    "values = result.get('values', [])\n",
    "\n",
    "df = pd.DataFrame(values)\n",
    "\n",
    "# replaces first row of dataframe with the headers b/c it's not automatic\n",
    "headers = df.iloc[0]\n",
    "df = df[1:]\n",
    "df.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreign_policy():\n",
    "    html = url.urlopen('https://foreignpolicy.com/category/latest/')\n",
    "    data = soup(html, 'html.parser')\n",
    "    entries = list(data.find(\"div\", class_=\"blog-list\").children)\n",
    "    \n",
    "    # foreign policy does this weird thing where they have one of the\n",
    "    #    class's children as '\\n' instead of just padding each entry.\n",
    "    def remove_newlines(list_):\n",
    "        filtered = list()\n",
    "        for item in list_:\n",
    "            if item == '\\n':\n",
    "                pass\n",
    "            else:\n",
    "                filtered.append(item)\n",
    "        return filtered\n",
    "    \n",
    "    entries = remove_newlines(entries)\n",
    "    \n",
    "    # TODO: Verification function that checks list against \n",
    "    \n",
    "    print(f\"There are {len(entries)} posts on Foreign Policy's latest page.\")\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19 posts on Foreign Policy's latest page.\n"
     ]
    }
   ],
   "source": [
    "# I'm making the 'get_foreign_policy' and 'extract_fp_data' separate for\n",
    "#    now because it's necessary to experiment on individual entries at the\n",
    "#    moment.\n",
    "new_posts = get_foreign_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scrape commands and helper scripts are specific to a website's layout\n",
    "class ForeignPolicyData(object):\n",
    "    \n",
    "    def __init__(self, post):\n",
    "        self.link = post.find_all(\"a\")[1].get('href')\n",
    "        self.title = post.h3.text\n",
    "        self.subtitle = post.p.text\n",
    "        self.date = self.get_date(self.link)\n",
    "        self.author = re.sub(r'[\\ \\n]{2,}', \n",
    "                             '', \n",
    "                             post.find_all(\"a\", class_='author')[0].text)\n",
    "    \n",
    "    # the date's a bit complicated because it's not on the first page\n",
    "    #    and contains significant formatting that interferes with the \n",
    "    #    the scrape.\n",
    "    def get_date(self, url_):\n",
    "        date = soup(url.urlopen(url_), 'html.parser').find_all(\"time\")[0].text\n",
    "\n",
    "        def clean_date(date):\n",
    "            regex = re.sub(r'\\n', '', date)\n",
    "            regex = re.sub(r',', '', regex)\n",
    "            regex = re.sub(r'\\s*..:..\\s..', '', regex)\n",
    "            return regex\n",
    "\n",
    "        date = clean_date(date)\n",
    "        date = datetime.strftime(datetime.strptime(date, '%B %d %Y'), '%m/%d/%Y')\n",
    "        return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtlasData(object):\n",
    "    \n",
    "    # assumes that a post is a Beautiful Soup object\n",
    "    def __init__(self, article, site=None):\n",
    "        \n",
    "        self.site = site\n",
    "        \n",
    "        if self.site == 'Foreign Policy':\n",
    "            article = ForeignPolicyData(article)\n",
    "        \n",
    "        # information for the database\n",
    "        self.link = article.link\n",
    "        self.title = article.title\n",
    "        self.subtitle = article.subtitle\n",
    "        self.date = article.date\n",
    "        self.author = article.author\n",
    "        \n",
    "        # assigned separate from instantiation in second round\n",
    "        self.geo_class = \"\"\n",
    "        self.region = \"\"\n",
    "        self.bilateral = \"\"\n",
    "        self.country = \"\"\n",
    "        \n",
    "        # assigned separate from instantiation in third round\n",
    "        self.topic = \"\"\n",
    "        self.subsection = \"\"\n",
    "        self.tone = \"\"\n",
    "        self.theme = \"\"\n",
    "        self.subtheme = \"\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.site}\\n\\n{self.title}\\n{self.subtitle}\\n{self.date}\\n{self.country}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of AtlasPost objects\n",
    "database_entries = list()\n",
    "for post in new_posts:\n",
    "    try:\n",
    "        database_entries.append(AtlasData(post, site='Foreign Policy'))\n",
    "    # haven't figured out a way to get it to acknowledge if an \n",
    "    #   attribute isn't present or it hits the end of the posts\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(database_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_entries(database_entries):\n",
    "    not_fp = list()\n",
    "    invalid = list()\n",
    "    single = list()\n",
    "    multiple = list()\n",
    "    \n",
    "    print(len(database_entries))\n",
    "    for entry in database_entries:\n",
    "        _class, country = assign_geo_class(get_gpes(entry.title), df)\n",
    "        if _class == 'invalid':\n",
    "            invalid.append(entry)\n",
    "        elif _class == 'single':\n",
    "            single.append(entry)\n",
    "        elif _class == 'multiple':\n",
    "            multiple.append(entry)\n",
    "        else:\n",
    "            not_fp.append(entry)\n",
    "        \n",
    "    \n",
    "    print(f\"There are {len(not_fp)} articles that were not Foreign Policy-related, \\n \\\n",
    "            There are {len(invalid)} articles that were classified as invalid, \\n \\\n",
    "            There are {len(single)} articles that were classified as single, \\n \\\n",
    "            There are {len(multiple)} articles that were classified as multiple.\")\n",
    "    \n",
    "    return not_fp, invalid, single, multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "There are 9 articles that were not Foreign Policy-related, \n",
      "             There are 3 articles that were classified as invalid, \n",
      "             There are 4 articles that were classified as single, \n",
      "             There are 2 articles that were classified as multiple.\n"
     ]
    }
   ],
   "source": [
    "not_fp, invalid, single, multiple = sort_entries(database_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
